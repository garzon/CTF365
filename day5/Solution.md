# Day 5

## Solution
Robots exclusion protocol is used by a website in order to notify web crawlers (like search engines) which parts of the site are not allowed to be visited and crawled.

The parts of a website which an admin doesn't want other people to visit often contain important information. So it's usually useful to see what's in `robots.txt`.

In this problem, you can find a "top secret" in `robots.txt`. The url is just what we need.
